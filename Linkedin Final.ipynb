{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf6367d9-64a2-4106-a280-b5dfe5a4827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 18:55:12,566 - ERROR - Failed to load BERT model: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\n",
      "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the LinkedIn company URL (e.g., https://www.linkedin.com/company/yahoo/):  https://www.linkedin.com/company/cybrisk-cyber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 18:55:28,489 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping company data for authenticity check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 18:55:29,380 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-14 18:55:29,444 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-14 18:55:29,502 - INFO - Driver [C:\\Users\\AnushaM\\.wdm\\drivers\\chromedriver\\win64\\136.0.7103.92\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-05-14 18:55:30,656 - INFO - Navigating to https://www.linkedin.com/company/cybrisk-cyber\n",
      "2025-05-14 18:55:56,222 - INFO - Extracted followers: 1,569 followers\n",
      "2025-05-14 18:55:57,225 - INFO - Extracted post 1: We‚Äôre thrilled to share that Cybrisk is now offici... (Timestamp: , Comments: 1 Comment)\n",
      "2025-05-14 18:55:57,226 - INFO - Extracted post 2: We‚Äôre thrilled to share that Cybrisk is now offici... (Timestamp: , Comments: 1 Comment)\n",
      "2025-05-14 18:55:57,229 - INFO - Extracted post 3: In today‚Äôs digital world, phishing attacks and fra... (Timestamp: , Comments: 1 Comment)\n",
      "2025-05-14 18:55:57,231 - INFO - Extracted post 4: Cyber Awareness vs. Cybersecurity ‚Äì Are You Truly ... (Timestamp: , Comments: 1 Comment)\n",
      "2025-05-14 18:55:57,233 - INFO - Extracted post 5: Losing Control of Your Digital World?¬†\n",
      "Hackers, de... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,234 - INFO - Extracted post 6: Is Your Business at Risk? Don‚Äôt Let Poor Cyber Hyg... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,237 - INFO - Extracted post 7: TAKE CONTROL with CTRL FAKE \n",
      "The digital world is ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,239 - INFO - Extracted post 8: Hooked by a Phishing Email? Not Anymore! \n",
      "Cybercri... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,240 - INFO - Extracted post 9: Deepfakes are getting smarter, but so are we! CTRL... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,243 - INFO - Extracted post 10: Stay One Step Ahead of Cyber Threats!\n",
      "CTRL Threats... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,245 - INFO - Extracted post 11: Spot the Fake, Secure the Truth!\n",
      "Not everything yo... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,247 - INFO - Extracted post 12: Phishing scams are evolving‚Äîare you protected?¬†\n",
      "Cy... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,249 - INFO - Extracted post 13: Detect It to Stop It!\n",
      "In an era of AI-driven conte... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,253 - INFO - Extracted post 14: Cybrisk recently launched two innovative platforms... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,256 - INFO - Extracted post 15: Shield Your Digital Presence!\n",
      "Cyber threats are ev... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,258 - INFO - Extracted post 16: The Connection is Virtual. The Danger is Real!\n",
      "\n",
      "De... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,260 - INFO - Extracted post 17: Introducing Ctrl Threats ‚Äì A Product of Cybrisk!\n",
      "Y... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,263 - INFO - Extracted post 18: Fake profiles are more than just an annoyance‚Äîthey... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,264 - INFO - Extracted post 19: Fake profiles aren‚Äôt just harmless pranks‚Äîthey‚Äôre ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,266 - INFO - Extracted post 20: As technology evolves, so do the tactics of cyberc... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,267 - INFO - Extracted post 21: Revealing the hidden dangers of deepfakes and how ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,269 - INFO - Extracted post 22: Data Breach vs. Business Growth: What Side Are You... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,272 - INFO - Extracted post 23: New Year, New Threats, Stronger Defenses!\n",
      "\n",
      "As we s... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,274 - INFO - Extracted post 24: What is VAPT (Part - 1), and why does your busines... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,278 - INFO - Extracted post 25: üéÑ This Christmas, unwrap peace of mind!\n",
      "\n",
      "While you... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,285 - INFO - Extracted post 26: Cyber threats are evolving‚Äîare you prepared?\n",
      "\n",
      "From... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,290 - INFO - Extracted post 27: Cybersecurity isn't just a shield; it's your busin... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,292 - INFO - Extracted post 28: Cybersecurity isn't just a shield; it's your busin... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,294 - INFO - Extracted post 29: üîç Are deepfakes putting your business at risk?\n",
      "Fro... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,296 - INFO - Extracted post 30: üìå Cyber Fact of the Day: Over 90% of data breaches... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,298 - INFO - Extracted post 31: From deepfake scams impersonating trusted leaders ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,300 - INFO - Extracted post 32: In today‚Äôs digital landscape, every business faces... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,303 - INFO - Extracted post 33: Cybrisk is your trusted partner in cybersecurity, ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,306 - INFO - Extracted post 34: As highlighted by Nithin Kamath , Founder & CEO of... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,308 - INFO - Extracted post 35: Cybrisk is your trusted partner in cybersecurity, ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,310 - INFO - Extracted post 36: Understanding AI-Generated Threats: From Fake Vide... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,313 - INFO - Extracted post 37: The Rise of Deepfake Technology: What You Need to ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,315 - INFO - Extracted post 38: Can You Stop Deepfake Attacks? A Guide to Preventi... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,318 - INFO - Extracted post 39: Stopping Deepfake Attacks in Real-Time: AI-Powered... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,320 - INFO - Extracted post 40: AI Audio Verification: Can You Trust What You Hear... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,322 - INFO - Extracted post 41: AI Detector vs. Fraudster: How to Track and Contro... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,326 - INFO - Extracted post 42: AI and Data Security: Protecting Against Deepfake ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,328 - INFO - Extracted post 43: Outsource Your Defense Against AI-Generated Media ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,331 - INFO - Extracted post 44: Detecting and Combating AI-Generated Voice Manipul... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,333 - INFO - Extracted post 45: Mastering Visual Information Verification in 2024 ... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,335 - INFO - Extracted post 46: Deepfakes and their impact on Indian society:\n",
      "Unde... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,337 - INFO - Extracted post 47: Deepfakes: A Silent Threat to Our Privacy\n",
      "\n",
      "In toda... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,340 - INFO - Extracted post 48: Discover How CtrlFake‚Äôs AI Solutions Are Revolutio... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,342 - INFO - Extracted post 49: Navigating¬†the¬†Complex¬†World¬†of¬†Deepfakes: \n",
      "\n",
      "What¬†... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,347 - INFO - Extracted post 50: A¬†New¬†Era¬†of¬†Deception:¬†Understanding¬†Deepfakes¬†an... (Timestamp: , Comments: 0 Comments)\n",
      "2025-05-14 18:55:57,348 - INFO - ====== WebDriver manager ======\n",
      "2025-05-14 18:55:58,774 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-14 18:55:58,833 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-14 18:55:58,877 - INFO - Driver [C:\\Users\\AnushaM\\.wdm\\drivers\\chromedriver\\win64\\136.0.7103.92\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-05-14 18:56:00,379 - INFO - Navigating to https://www.linkedin.com/jobs/search/?keywords=cybrisk cyber%20jobs\n",
      "2025-05-14 18:56:15,249 - INFO - No cookie consent prompt found\n",
      "2025-05-14 18:56:15,262 - INFO - Scrolling to load job listings\n",
      "2025-05-14 18:56:25,340 - WARNING - Job list not found during scroll\n",
      "2025-05-14 18:56:25,389 - ERROR - No job listings found.\n",
      "2025-05-14 18:56:29,204 - WARNING - Website returned status code 403\n",
      "2025-05-14 18:56:29,205 - INFO - Raw follower text: '1,569 followers'\n",
      "2025-05-14 18:56:29,205 - INFO - Parsed followers count: 1569\n",
      "2025-05-14 18:56:29,206 - INFO - Post 1: Reactions=10, Comments=1, Commentary='We‚Äôre thrilled to share that Cybrisk is now offici...'\n",
      "2025-05-14 18:56:29,207 - INFO - Post 2: Reactions=11, Comments=1, Commentary='We‚Äôre thrilled to share that Cybrisk is now offici...'\n",
      "2025-05-14 18:56:29,207 - INFO - Post 3: Reactions=8, Comments=1, Commentary='In today‚Äôs digital world, phishing attacks and fra...'\n",
      "2025-05-14 18:56:29,209 - INFO - Post 4: Reactions=11, Comments=1, Commentary='Cyber Awareness vs. Cybersecurity ‚Äì Are You Truly ...'\n",
      "2025-05-14 18:56:29,209 - INFO - Post 5: Reactions=8, Comments=0, Commentary='Losing Control of Your Digital World?¬†\n",
      "Hackers, de...'\n",
      "2025-05-14 18:56:29,210 - INFO - Post 6: Reactions=17, Comments=0, Commentary='Is Your Business at Risk? Don‚Äôt Let Poor Cyber Hyg...'\n",
      "2025-05-14 18:56:29,211 - INFO - Post 7: Reactions=11, Comments=0, Commentary='TAKE CONTROL with CTRL FAKE \n",
      "The digital world is ...'\n",
      "2025-05-14 18:56:29,211 - INFO - Post 8: Reactions=10, Comments=0, Commentary='Hooked by a Phishing Email? Not Anymore! \n",
      "Cybercri...'\n",
      "2025-05-14 18:56:29,212 - INFO - Post 9: Reactions=6, Comments=0, Commentary='Deepfakes are getting smarter, but so are we! CTRL...'\n",
      "2025-05-14 18:56:29,213 - INFO - Post 10: Reactions=13, Comments=0, Commentary='Stay One Step Ahead of Cyber Threats!\n",
      "CTRL Threats...'\n",
      "2025-05-14 18:56:29,213 - INFO - Post 11: Reactions=8, Comments=0, Commentary='Spot the Fake, Secure the Truth!\n",
      "Not everything yo...'\n",
      "2025-05-14 18:56:29,214 - INFO - Post 12: Reactions=10, Comments=0, Commentary='Phishing scams are evolving‚Äîare you protected?¬†\n",
      "Cy...'\n",
      "2025-05-14 18:56:29,215 - INFO - Post 13: Reactions=8, Comments=0, Commentary='Detect It to Stop It!\n",
      "In an era of AI-driven conte...'\n",
      "2025-05-14 18:56:29,216 - INFO - Post 14: Reactions=23, Comments=0, Commentary='Cybrisk recently launched two innovative platforms...'\n",
      "2025-05-14 18:56:29,217 - INFO - Post 15: Reactions=8, Comments=0, Commentary='Shield Your Digital Presence!\n",
      "Cyber threats are ev...'\n",
      "2025-05-14 18:56:29,217 - INFO - Post 16: Reactions=10, Comments=0, Commentary='The Connection is Virtual. The Danger is Real!\n",
      "\n",
      "De...'\n",
      "2025-05-14 18:56:29,218 - INFO - Post 17: Reactions=11, Comments=0, Commentary='Introducing Ctrl Threats ‚Äì A Product of Cybrisk!\n",
      "Y...'\n",
      "2025-05-14 18:56:29,219 - INFO - Post 18: Reactions=8, Comments=0, Commentary='Fake profiles are more than just an annoyance‚Äîthey...'\n",
      "2025-05-14 18:56:29,219 - INFO - Post 19: Reactions=6, Comments=0, Commentary='Fake profiles aren‚Äôt just harmless pranks‚Äîthey‚Äôre ...'\n",
      "2025-05-14 18:56:29,220 - INFO - Post 20: Reactions=4, Comments=0, Commentary='As technology evolves, so do the tactics of cyberc...'\n",
      "2025-05-14 18:56:29,221 - INFO - Post 21: Reactions=9, Comments=0, Commentary='Revealing the hidden dangers of deepfakes and how ...'\n",
      "2025-05-14 18:56:29,221 - INFO - Post 22: Reactions=13, Comments=0, Commentary='Data Breach vs. Business Growth: What Side Are You...'\n",
      "2025-05-14 18:56:29,222 - INFO - Post 23: Reactions=3, Comments=0, Commentary='New Year, New Threats, Stronger Defenses!\n",
      "\n",
      "As we s...'\n",
      "2025-05-14 18:56:29,223 - INFO - Post 24: Reactions=5, Comments=0, Commentary='What is VAPT (Part - 1), and why does your busines...'\n",
      "2025-05-14 18:56:29,223 - INFO - Post 25: Reactions=7, Comments=0, Commentary='üéÑ This Christmas, unwrap peace of mind!\n",
      "\n",
      "While you...'\n",
      "2025-05-14 18:56:29,225 - INFO - Post 26: Reactions=5, Comments=0, Commentary='Cyber threats are evolving‚Äîare you prepared?\n",
      "\n",
      "From...'\n",
      "2025-05-14 18:56:29,226 - INFO - Post 27: Reactions=11, Comments=0, Commentary='Cybersecurity isn't just a shield; it's your busin...'\n",
      "2025-05-14 18:56:29,226 - INFO - Post 28: Reactions=11, Comments=0, Commentary='Cybersecurity isn't just a shield; it's your busin...'\n",
      "2025-05-14 18:56:29,227 - INFO - Post 29: Reactions=9, Comments=0, Commentary='üîç Are deepfakes putting your business at risk?\n",
      "Fro...'\n",
      "2025-05-14 18:56:29,227 - INFO - Post 30: Reactions=11, Comments=0, Commentary='üìå Cyber Fact of the Day: Over 90% of data breaches...'\n",
      "2025-05-14 18:56:29,228 - INFO - Post 31: Reactions=16, Comments=0, Commentary='From deepfake scams impersonating trusted leaders ...'\n",
      "2025-05-14 18:56:29,228 - INFO - Post 32: Reactions=20, Comments=0, Commentary='In today‚Äôs digital landscape, every business faces...'\n",
      "2025-05-14 18:56:29,229 - INFO - Post 33: Reactions=14, Comments=0, Commentary='Cybrisk is your trusted partner in cybersecurity, ...'\n",
      "2025-05-14 18:56:29,229 - INFO - Post 34: Reactions=9, Comments=0, Commentary='As highlighted by Nithin Kamath , Founder & CEO of...'\n",
      "2025-05-14 18:56:29,230 - INFO - Post 35: Reactions=14, Comments=0, Commentary='Cybrisk is your trusted partner in cybersecurity, ...'\n",
      "2025-05-14 18:56:29,232 - INFO - Post 36: Reactions=4, Comments=0, Commentary='Understanding AI-Generated Threats: From Fake Vide...'\n",
      "2025-05-14 18:56:29,232 - INFO - Post 37: Reactions=3, Comments=0, Commentary='The Rise of Deepfake Technology: What You Need to ...'\n",
      "2025-05-14 18:56:29,233 - INFO - Post 38: Reactions=4, Comments=0, Commentary='Can You Stop Deepfake Attacks? A Guide to Preventi...'\n",
      "2025-05-14 18:56:29,234 - INFO - Post 39: Reactions=7, Comments=0, Commentary='Stopping Deepfake Attacks in Real-Time: AI-Powered...'\n",
      "2025-05-14 18:56:29,234 - INFO - Post 40: Reactions=3, Comments=0, Commentary='AI Audio Verification: Can You Trust What You Hear...'\n",
      "2025-05-14 18:56:29,235 - INFO - Post 41: Reactions=3, Comments=0, Commentary='AI Detector vs. Fraudster: How to Track and Contro...'\n",
      "2025-05-14 18:56:29,235 - INFO - Post 42: Reactions=3, Comments=0, Commentary='AI and Data Security: Protecting Against Deepfake ...'\n",
      "2025-05-14 18:56:29,236 - INFO - Post 43: Reactions=5, Comments=0, Commentary='Outsource Your Defense Against AI-Generated Media ...'\n",
      "2025-05-14 18:56:29,237 - INFO - Post 44: Reactions=2, Comments=0, Commentary='Detecting and Combating AI-Generated Voice Manipul...'\n",
      "2025-05-14 18:56:29,237 - INFO - Post 45: Reactions=2, Comments=0, Commentary='Mastering Visual Information Verification in 2024 ...'\n",
      "2025-05-14 18:56:29,239 - INFO - Post 46: Reactions=3, Comments=0, Commentary='Deepfakes and their impact on Indian society:\n",
      "Unde...'\n",
      "2025-05-14 18:56:29,240 - INFO - Post 47: Reactions=2, Comments=0, Commentary='Deepfakes: A Silent Threat to Our Privacy\n",
      "\n",
      "In toda...'\n",
      "2025-05-14 18:56:29,240 - INFO - Post 48: Reactions=1, Comments=0, Commentary='Discover How CtrlFake‚Äôs AI Solutions Are Revolutio...'\n",
      "2025-05-14 18:56:29,241 - INFO - Post 49: Reactions=3, Comments=0, Commentary='Navigating¬†the¬†Complex¬†World¬†of¬†Deepfakes: \n",
      "\n",
      "What¬†...'\n",
      "2025-05-14 18:56:29,242 - INFO - Post 50: Reactions=1, Comments=0, Commentary='A¬†New¬†Era¬†of¬†Deception:¬†Understanding¬†Deepfakes¬†an...'\n",
      "2025-05-14 18:56:29,243 - INFO - Total posts: 50, Total reactions: 404, Total comments: 4\n",
      "2025-05-14 18:56:29,244 - INFO - Average engagement rate: 0.5201%\n",
      "2025-05-14 18:56:31,540 - WARNING - BERT model not available, skipping NLP analysis\n",
      "2025-05-14 18:56:31,542 - INFO - Raw follower text: '1,569 followers'\n",
      "2025-05-14 18:56:31,544 - INFO - Raw follower text: '1,569 followers'\n",
      "2025-05-14 18:56:31,546 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating authenticity...\n",
      "Scraping leaders, and affiliated pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 18:56:32,457 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-14 18:56:32,517 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-14 18:56:32,603 - INFO - Driver [C:\\Users\\AnushaM\\.wdm\\drivers\\chromedriver\\win64\\136.0.7103.92\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-05-14 18:56:33,859 - INFO - Navigating to https://www.linkedin.com/company/cybrisk-cyber/life\n",
      "2025-05-14 18:56:57,921 - INFO - Extracted affiliated page: CTRL THREATS\n",
      "2025-05-14 18:56:57,922 - INFO - Extracted affiliated page: CTRL FAKE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analysis for F9 Cybrisk Tech Private Limited ===\n",
      "LinkedIn URL: https://www.linkedin.com/company/cybrisk-cyber\n",
      "\n",
      "--- Authenticity Check Results ---\n",
      "Company Type: Startup\n",
      "Founded: 2024\n",
      "Followers: 1,569 followers\n",
      "Employee Count: 11-50 employees\n",
      "Job Count: \n",
      "Post Count: 50\n",
      "Average Engagement Rate: 0.5201%\n",
      "Average Reactions per Post: 8.08\n",
      "Average Comments per Post: 0.08\n",
      "Website Authenticity Score: 0/20\n",
      "Website Professional: False\n",
      "Website Has Contact Info: False\n",
      "Website Has Red Flags: False\n",
      "Website Broken Links: 0\n",
      "Authenticity Score: 75/150\n",
      "Is Likely Authentic: True\n",
      "Results saved to f9_cybrisk_tech_private_limited_company_data.json\n",
      "\n",
      "Company analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import whois\n",
    "import ssl\n",
    "import socket\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize BERT sentiment analysis pipeline\n",
    "try:\n",
    "    sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load BERT model: {e}\")\n",
    "    sentiment_analyzer = None\n",
    "\n",
    "# Function to validate LinkedIn URL\n",
    "def validate_linkedin_url(url):\n",
    "    pattern = r'^https://www\\.linkedin\\.com/company/[\\w-]+/?$'\n",
    "    return bool(re.match(pattern, url))\n",
    "\n",
    "# Function to parse follower count\n",
    "def parse_followers_count(text):\n",
    "    try:\n",
    "        if not text:\n",
    "            logging.warning(\"Follower text is empty\")\n",
    "            return 0\n",
    "        logging.info(f\"Raw follower text: '{text}'\")\n",
    "        text = text.strip().lower().replace('followers', '').replace(',', '').replace('+', '').strip()\n",
    "        match = re.match(r'(\\d+\\.?\\d*)\\s*([mk]?)', text)\n",
    "        if match:\n",
    "            number = float(match.group(1))\n",
    "            unit = match.group(2)\n",
    "            if unit == 'm':\n",
    "                return int(number * 1_000_000)\n",
    "            elif unit == 'k':\n",
    "                return int(number * 1_000)\n",
    "            return int(number)\n",
    "        num = re.search(r'\\d+', text)\n",
    "        if num:\n",
    "            return int(num.group())\n",
    "        logging.warning(f\"Could not parse follower count from: '{text}'\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing followers count '{text}': {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to parse reactions or comments\n",
    "def parse_reaction_count(text):\n",
    "    try:\n",
    "        if not text:\n",
    "            return 0\n",
    "        text = text.strip().lower().replace('reactions', '').replace('likes', '').replace('comments', '').replace('comment', '').replace(',', '').strip()\n",
    "        match = re.match(r'(\\d+\\.?\\d*)\\s*([mk]?)', text)\n",
    "        if match:\n",
    "            number = float(match.group(1))\n",
    "            unit = match.group(2)\n",
    "            if unit == 'm':\n",
    "                return int(number * 1_000_000)\n",
    "            elif unit == 'k':\n",
    "                return int(number * 1_000)\n",
    "            return int(number)\n",
    "        num = re.search(r'\\d+', text)\n",
    "        if num:\n",
    "            return int(num.group())\n",
    "        logging.warning(f\"Could not parse reaction count from: '{text}'\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing reaction count '{text}': {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to check if job is within past 1 month\n",
    "def is_job_recent(posted):\n",
    "    if not posted:\n",
    "        return False\n",
    "    posted = posted.lower()\n",
    "    if 'hour' in posted or 'day' in posted:\n",
    "        return True\n",
    "    if 'week' in posted:\n",
    "        try:\n",
    "            weeks = int(re.search(r'\\d+', posted).group())\n",
    "            return weeks <= 4\n",
    "        except:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# Function to check WHOIS information\n",
    "def check_whois(domain):\n",
    "    try:\n",
    "        w = whois.whois(domain)\n",
    "        whois_data = {\n",
    "            'domain_name': w.domain_name,\n",
    "            'registrar': w.registrar,\n",
    "            'creation_date': w.creation_date.isoformat() if isinstance(w.creation_date, datetime.datetime) else w.creation_date,\n",
    "            'expiration_date': w.expiration_date.isoformat() if isinstance(w.expiration_date, datetime.datetime) else w.expiration_date,\n",
    "            'last_updated': w.last_updated.isoformat() if isinstance(w.last_updated, datetime.datetime) else w.last_updated,\n",
    "            'name_servers': w.name_servers\n",
    "        }\n",
    "        if isinstance(whois_data['domain_name'], list):\n",
    "            whois_data['domain_name'] = whois_data['domain_name'][0] if whois_data['domain_name'] else None\n",
    "        if isinstance(whois_data['creation_date'], list):\n",
    "            whois_data['creation_date'] = whois_data['creation_date'][0].isoformat() if whois_data['creation_date'] else None\n",
    "        if isinstance(whois_data['expiration_date'], list):\n",
    "            whois_data['expiration_date'] = whois_data['expiration_date'][0].isoformat() if whois_data['expiration_date'] else None\n",
    "        if isinstance(whois_data['last_updated'], list):\n",
    "            whois_data['last_updated'] = whois_data['last_updated'][0].isoformat() if whois_data['last_updated'] else None\n",
    "        return whois_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"WHOIS check failed for {domain}: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Function to check SSL certificate\n",
    "def check_ssl(domain):\n",
    "    try:\n",
    "        context = ssl.create_default_context()\n",
    "        with socket.create_connection((domain, 443)) as sock:\n",
    "            with context.wrap_socket(sock, server_hostname=domain) as ssock:\n",
    "                cert = ssock.getpeercert()\n",
    "                return {\n",
    "                    'issuer': dict(x[0] for x in cert['issuer']),\n",
    "                    'subject': dict(x[0] for x in cert['subject']),\n",
    "                    'not_before': cert['notBefore'],\n",
    "                    'not_after': cert['notAfter'],\n",
    "                    'serial_number': cert['serialNumber']\n",
    "                }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SSL check failed for {domain}: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Function to check HTTPS status\n",
    "def check_https(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        return {\n",
    "            'is_https': response.url.startswith('https://'),\n",
    "            'status_code': response.status_code,\n",
    "            'response_time': response.elapsed.total_seconds()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"HTTPS check failed for {url}: {e}\")\n",
    "        return {'error': str(e), 'is_https': False, 'status_code': None, 'response_time': None}\n",
    "\n",
    "# Function to calculate domain age\n",
    "def calculate_domain_age(whois_data):\n",
    "    try:\n",
    "        creation_date = whois_data.get('creation_date')\n",
    "        if not creation_date:\n",
    "            return {'error': 'No creation date found'}\n",
    "        \n",
    "        if isinstance(creation_date, str):\n",
    "            try:\n",
    "                creation_date = datetime.datetime.fromisoformat(creation_date.replace('Z', ''))\n",
    "            except ValueError:\n",
    "                match = re.search(r'\\d{4}-\\d{2}-\\d{2}', creation_date)\n",
    "                if match:\n",
    "                    creation_date = datetime.datetime.strptime(match.group(0), '%Y-%m-%d')\n",
    "                else:\n",
    "                    return {'error': 'Invalid creation date format'}\n",
    "        \n",
    "        current_date = datetime.datetime.now()\n",
    "        age = current_date - creation_date\n",
    "        return {\n",
    "            'age_days': age.days,\n",
    "            'age_years': round(age.days / 365.25, 2)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Domain age calculation failed: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Function to check website authenticity\n",
    "def check_website_authenticity(url, company_name):\n",
    "    try:\n",
    "        result = {\n",
    "            'is_professional': False,\n",
    "            'has_contact_info': False,\n",
    "            'has_red_flags': False,\n",
    "            'broken_links_count': 0,\n",
    "            'response_time': None,\n",
    "            'authenticity_score': 0\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "        result['response_time'] = response.elapsed.total_seconds()\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logging.warning(f\"Website returned status code {response.status_code}\")\n",
    "            return result\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        text_content = soup.get_text().lower()\n",
    "        professional_indicators = [\n",
    "            company_name.lower(),\n",
    "            'about us', 'services', 'products', 'team', 'careers', 'contact',\n",
    "            'privacy policy', 'terms of service'\n",
    "        ]\n",
    "        result['is_professional'] = any(indicator in text_content for indicator in professional_indicators)\n",
    "        if result['is_professional']:\n",
    "            result['authenticity_score'] += 5\n",
    "        \n",
    "        contact_indicators = [\n",
    "            'email:', 'phone:', 'tel:', 'contact us', 'get in touch',\n",
    "            re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'),\n",
    "            re.compile(r'\\b\\d{3}-\\d{3}-\\d{4}\\b')\n",
    "        ]\n",
    "        for indicator in contact_indicators:\n",
    "            if isinstance(indicator, str):\n",
    "                if indicator in text_content:\n",
    "                    result['has_contact_info'] = True\n",
    "                    break\n",
    "            else:\n",
    "                if indicator.search(text_content):\n",
    "                    result['has_contact_info'] = True\n",
    "                    break\n",
    "        if result['has_contact_info']:\n",
    "            result['authenticity_score'] += 5\n",
    "        \n",
    "        red_flags = ['lorem ipsum', 'under construction', 'coming soon']\n",
    "        result['has_red_flags'] = any(flag in text_content for flag in red_flags)\n",
    "        if not result['has_red_flags']:\n",
    "            result['authenticity_score'] += 5\n",
    "        \n",
    "        links = [a.get('href') for a in soup.find_all('a') if a.get('href')]\n",
    "        broken_links = 0\n",
    "        for link in links[:10]:\n",
    "            if link.startswith('#') or not link.startswith('http'):\n",
    "                continue\n",
    "            try:\n",
    "                link_response = requests.head(link, timeout=5, allow_redirects=True)\n",
    "                if link_response.status_code >= 400:\n",
    "                    broken_links += 1\n",
    "            except:\n",
    "                broken_links += 1\n",
    "        result['broken_links_count'] = broken_links\n",
    "        if broken_links == 0:\n",
    "            result['authenticity_score'] += 5\n",
    "        \n",
    "        logging.info(f\"Website authenticity result: {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Website authenticity check failed for {url}: {e}\")\n",
    "        return {\n",
    "            'is_professional': False,\n",
    "            'has_contact_info': False,\n",
    "            'has_red_flags': True,\n",
    "            'broken_links_count': 0,\n",
    "            'response_time': None,\n",
    "            'authenticity_score': 0\n",
    "        }\n",
    "\n",
    "# Function to calculate engagement rate per post and average\n",
    "def calculate_engagement_rate(posts, followers):\n",
    "    try:\n",
    "        if not posts or not followers:\n",
    "            logging.warning(\"No posts or followers provided\")\n",
    "            return {\n",
    "                'average_engagement_rate': 0.0,\n",
    "                'average_reactions': 0.0,\n",
    "                'average_comments': 0.0,\n",
    "                'per_post_engagement': []\n",
    "            }\n",
    "        \n",
    "        followers_count = parse_followers_count(followers)\n",
    "        logging.info(f\"Parsed followers count: {followers_count}\")\n",
    "        if followers_count == 0:\n",
    "            logging.warning(\"Followers count is zero, setting engagement to 0\")\n",
    "            return {\n",
    "                'average_engagement_rate': 0.0,\n",
    "                'average_reactions': 0.0,\n",
    "                'average_comments': 0.0,\n",
    "                'per_post_engagement': []\n",
    "            }\n",
    "        \n",
    "        total_reactions = 0\n",
    "        total_comments = 0\n",
    "        post_count = len(posts)\n",
    "        per_post_engagement = []\n",
    "        \n",
    "        for i, post in enumerate(posts):\n",
    "            reactions_count = parse_reaction_count(post['reactions'])\n",
    "            comments_count = parse_reaction_count(post['comments'])\n",
    "            \n",
    "            logging.info(f\"Post {i+1}: Reactions={reactions_count}, Comments={comments_count}, Commentary='{post['commentary'][:50]}...'\")\n",
    "            \n",
    "            engagement = (reactions_count + comments_count) / followers_count * 100\n",
    "            per_post_engagement.append({\n",
    "                'commentary': post['commentary'][:50] + '...' if len(post['commentary']) > 50 else post['commentary'],\n",
    "                'engagement_rate': round(engagement, 4),\n",
    "                'reactions': reactions_count,\n",
    "                'comments': comments_count\n",
    "            })\n",
    "            \n",
    "            total_reactions += reactions_count\n",
    "            total_comments += comments_count\n",
    "        \n",
    "        average_reactions = total_reactions / post_count if post_count > 0 else 0\n",
    "        average_comments = total_comments / post_count if post_count > 0 else 0\n",
    "        average_engagement_rate = sum(p['engagement_rate'] for p in per_post_engagement) / post_count if post_count > 0 else 0\n",
    "        \n",
    "        logging.info(f\"Total posts: {post_count}, Total reactions: {total_reactions}, Total comments: {total_comments}\")\n",
    "        logging.info(f\"Average engagement rate: {average_engagement_rate:.4f}%\")\n",
    "        \n",
    "        return {\n",
    "            'average_engagement_rate': round(average_engagement_rate, 4),\n",
    "            'average_reactions': round(average_reactions, 2),\n",
    "            'average_comments': round(average_comments, 2),\n",
    "            'per_post_engagement': per_post_engagement\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Engagement rate calculation failed: {e}\")\n",
    "        return {\n",
    "            'average_engagement_rate': 0.0,\n",
    "            'average_reactions': 0.0,\n",
    "            'average_comments': 0.0,\n",
    "            'per_post_engagement': []\n",
    "        }\n",
    "\n",
    "# Function to analyze posts using BERT\n",
    "def analyze_posts(posts):\n",
    "    results = []\n",
    "    if not sentiment_analyzer:\n",
    "        logging.warning(\"BERT model not available, skipping NLP analysis\")\n",
    "        return results\n",
    "    \n",
    "    try:\n",
    "        for post in posts:\n",
    "            text = post['commentary']\n",
    "            text = re.sub(r'http\\S+|#\\S+', '', text)\n",
    "            if len(text) > 512:\n",
    "                text = text[:512]\n",
    "            \n",
    "            bert_result = sentiment_analyzer(text)[0]\n",
    "            sentiment = bert_result['label'].lower()\n",
    "            sentiment_score = bert_result['score']\n",
    "            \n",
    "            word_count = len(text.split())\n",
    "            is_professional = word_count > 20 and sentiment_score > 0.7\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': 'positive' if sentiment == 'positive' else 'negative',\n",
    "                'sentiment_score': round(sentiment_score, 2),\n",
    "                'is_professional': is_professional\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"BERT analysis failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to scrape LinkedIn jobs\n",
    "def scrape_linkedin_company_jobs(company_name):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        jobs_data = {\n",
    "            'job_count': '',\n",
    "            'job_listings': []\n",
    "        }\n",
    "        \n",
    "        jobs_url = f\"https://www.linkedin.com/jobs/search/?keywords={company_name}%20jobs\"\n",
    "        logging.info(f\"Navigating to {jobs_url}\")\n",
    "        driver.get(jobs_url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            cookie_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept')]\"))\n",
    "            )\n",
    "            cookie_button.click()\n",
    "            logging.info(\"Accepted cookie consent prompt\")\n",
    "        except:\n",
    "            logging.info(\"No cookie consent prompt found\")\n",
    "        \n",
    "        login_prompt = driver.find_elements(By.CLASS_NAME, \"authwall-join-form\")\n",
    "        if login_prompt:\n",
    "            logging.warning(\"Login prompt detected. Job data may be restricted.\")\n",
    "            return jobs_data\n",
    "        \n",
    "        logging.info(\"Scrolling to load job listings\")\n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                job_list = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"ul.jobs-search__results-list\"))\n",
    "                )\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                logging.warning(\"Job list not found during scroll\")\n",
    "                break\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        jobs_list = soup.select('ul.jobs-search__results-list')\n",
    "        if not jobs_list:\n",
    "            logging.error(\"No job listings found.\")\n",
    "            return jobs_data\n",
    "        \n",
    "        job_count_elem = soup.select_one('span.results-context-header__job-count')\n",
    "        if job_count_elem:\n",
    "            jobs_data['job_count'] = job_count_elem.text.strip()\n",
    "            logging.info(f\"Found job count: {jobs_data['job_count']}\")\n",
    "        \n",
    "        job_counter = 0\n",
    "        for job in soup.select('ul.jobs-search__results-list > li'):\n",
    "            if job_counter >= 20:\n",
    "                break\n",
    "                \n",
    "            job_card = job.select_one('div[class*=\"base-card\"]')\n",
    "            if job_card:\n",
    "                title_elem = job_card.select_one('h3[class*=\"base-search-card__title\"]')\n",
    "                subtitle_elem = job_card.select_one('h4[class*=\"base-search-card__subtitle\"]')\n",
    "                location_elem = job_card.select_one('span.job-search-card__location')\n",
    "                time_elem = job_card.select_one('time[class*=\"job-search-card__listdate\"]')\n",
    "                link_elem = job_card.select_one('a[class*=\"base-card__full-link\"]')\n",
    "                               \n",
    "                posted = time_elem.text.strip() if time_elem else ''\n",
    "                if not is_job_recent(posted):\n",
    "                    continue\n",
    "                \n",
    "                job_data = {\n",
    "                    'title': title_elem.text.strip() if title_elem else '',\n",
    "                    'company': subtitle_elem.text.strip() if subtitle_elem else '',\n",
    "                    'location': location_elem.text.strip() if location_elem else '',\n",
    "                    'posted': posted,\n",
    "                    'url': link_elem['href'] if link_elem else '',\n",
    "                }\n",
    "                jobs_data['job_listings'].append(job_data)\n",
    "                logging.info(f\"Extracted job: {job_data['title']}\")\n",
    "                job_counter += 1\n",
    "        \n",
    "        return jobs_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {e}\")\n",
    "        return {'job_count': '', 'job_listings': []}\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "# Function to scrape LinkedIn company followers\n",
    "def scrape_linkedin_company_followers(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            followers_tag = soup.find('meta', {\"property\": \"og:description\"}) or \\\n",
    "                           soup.find('div', class_=re.compile('org-top-card-summary-info-list'))\n",
    "            \n",
    "            followers_count = \"0 followers\"\n",
    "            if followers_tag:\n",
    "                text = followers_tag.get('content', '') if followers_tag.name == 'meta' else followers_tag.get_text(strip=True)\n",
    "                followers_match = re.search(r'\\b(\\d[\\d,.]*)\\s+followers\\b', text, re.IGNORECASE)\n",
    "                if followers_match:\n",
    "                    followers_count = followers_match.group(1) + \" followers\"\n",
    "                else:\n",
    "                    followers_alt = soup.find('span', class_='org-top-card-summary__follower-count')\n",
    "                    if followers_alt:\n",
    "                        followers_count = followers_alt.text.strip()\n",
    "            logging.info(f\"Extracted followers: {followers_count}\")\n",
    "            return followers_count\n",
    "        else:\n",
    "            logging.warning(f\"Unable to retrieve LinkedIn company page. Status code: {response.status_code}\")\n",
    "            return \"0 followers\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping followers: {str(e)}\")\n",
    "        return \"0 followers\"\n",
    "\n",
    "# Function to scrape LinkedIn company data from URL\n",
    "def scrape_linkedin_company_from_url(linkedin_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        company_name = linkedin_url.split('/company/')[-1].split('/')[0].replace('-', ' ')\n",
    "        \n",
    "        company_data = {\n",
    "            'overview': {\n",
    "                'name': '',\n",
    "                'sector': '',\n",
    "                'location': '',\n",
    "                'followers': '',\n",
    "                'website': '',\n",
    "                'industry': '',\n",
    "                'company_size': '',\n",
    "                'headquarters': '',\n",
    "                'founded': '',\n",
    "                'locations': []\n",
    "            },\n",
    "            'jobs': {\n",
    "                'job_count': '',\n",
    "                'job_listings': []\n",
    "            },\n",
    "            'posts': [],\n",
    "            'domain_info': {\n",
    "                'whois': {},\n",
    "                'ssl': {},\n",
    "                'https': {},\n",
    "                'domain_age': {},\n",
    "                'website_authenticity': {},\n",
    "                'engagement': {}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Navigating to {linkedin_url}\")\n",
    "        driver.get(linkedin_url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        name_elem = soup.find('h1', class_='top-card-layout__title')\n",
    "        if name_elem:\n",
    "            company_data['overview']['name'] = name_elem.text.strip()\n",
    "        \n",
    "        sector_elem = soup.find('h2', class_='top-card-layout__headline')\n",
    "        if sector_elem:\n",
    "            company_data['overview']['sector'] = sector_elem.text.strip()\n",
    "        \n",
    "        company_data['overview']['followers'] = scrape_linkedin_company_followers(linkedin_url)\n",
    "        \n",
    "        website_elem = soup.find('div', attrs={'data-test-id': 'about-us__website'})\n",
    "        if website_elem:\n",
    "            website_link = website_elem.find('a')\n",
    "            if website_link:\n",
    "                company_data['overview']['website'] = website_link.text.strip()\n",
    "        \n",
    "        industry_elem = soup.find('div', attrs={'data-test-id': 'about-us__industry'})\n",
    "        if industry_elem:\n",
    "            industry_dd = industry_elem.find('dd')\n",
    "            if industry_dd:\n",
    "                company_data['overview']['industry'] = industry_dd.text.strip()\n",
    "        \n",
    "        size_elem = soup.find('div', attrs={'data-test-id': 'about-us__size'})\n",
    "        if size_elem:\n",
    "            size_dd = size_elem.find('dd')\n",
    "            if size_dd:\n",
    "                company_data['overview']['company_size'] = size_dd.text.strip()\n",
    "        \n",
    "        hq_elem = soup.find('div', attrs={'data-test-id': 'about-us__headquarters'})\n",
    "        if hq_elem:\n",
    "            hq_dd = hq_elem.find('dd')\n",
    "            if hq_dd:\n",
    "                company_data['overview']['headquarters'] = hq_dd.text.strip()\n",
    "        \n",
    "        founded_elem = soup.find('div', attrs={'data-test-id': 'about-us__foundedOn'})\n",
    "        if founded_elem:\n",
    "            founded_dd = founded_elem.find('dd')\n",
    "            if founded_dd:\n",
    "                company_data['overview']['founded'] = founded_dd.text.strip()\n",
    "        \n",
    "        try:\n",
    "            see_all_locations = driver.find_elements(By.XPATH, \"//button[contains(text(), 'See all')]\")\n",
    "            for button in see_all_locations:\n",
    "                if 'locations' in button.get_attribute('aria-label').lower():\n",
    "                    driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    time.sleep(3)\n",
    "                    break\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            locations_list = soup.find('ul', class_='show-more-less__list')\n",
    "            if locations_list:\n",
    "                for li in locations_list.find_all('li'):\n",
    "                    address_div = li.find('div', id=lambda x: x and x.startswith('address-'))\n",
    "                    if address_div:\n",
    "                        address_parts = [p.text.strip() for p in address_div.find_all('p')]\n",
    "                        address = ', '.join(address_parts)\n",
    "                        is_primary = bool(li.find('span', class_='tag-sm tag-enabled'))\n",
    "                        company_data['overview']['locations'].append({\n",
    "                            'address': address,\n",
    "                            'is_primary': is_primary\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting locations: {e}\")\n",
    "        \n",
    "        try:\n",
    "            see_all_posts = driver.find_elements(By.XPATH, \"//button[contains(text(), 'See all')]\")\n",
    "            for button in see_all_posts:\n",
    "                if 'posts' in button.get_attribute('aria-label').lower():\n",
    "                    driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    time.sleep(5)\n",
    "                    for _ in range(10):\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        time.sleep(3)\n",
    "                        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        if new_height == last_height:\n",
    "                            break\n",
    "                        last_height = new_height\n",
    "                    break\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            comment_count_script = \"\"\"\n",
    "            function extractCommentCounts() {\n",
    "                const commentElements = document.querySelectorAll('[data-test-id=\"social-actions__comments\"]');\n",
    "                const commentCounts = [];\n",
    "                commentElements.forEach((element, index) => {\n",
    "                    const commentCount = parseInt(element.getAttribute('data-num-comments'), 10);\n",
    "                    commentCounts.push({\n",
    "                        postIndex: index + 1,\n",
    "                        commentCount: isNaN(commentCount) ? 0 : commentCount\n",
    "                    });\n",
    "                });\n",
    "                return commentCounts;\n",
    "            }\n",
    "            return extractCommentCounts();\n",
    "            \"\"\"\n",
    "            comment_counts = driver.execute_script(comment_count_script)\n",
    "            comment_counts_dict = {item['postIndex']: item['commentCount'] for item in comment_counts}\n",
    "            \n",
    "            posts_list = soup.find_all('article', class_='main-feed-activity-card')\n",
    "            for i, post in enumerate(posts_list[:50]):\n",
    "                commentary = post.find('p', class_='attributed-text-segment-list__content')\n",
    "                reactions = post.find('span', attrs={'data-test-id': 'social-actions__reaction-count'})\n",
    "                if not reactions:\n",
    "                    reactions = post.find('span', class_='social-details-social-counts__reactions-count')\n",
    "                comments = post.find('a', attrs={'data-test-id': 'social-actions__comments'})\n",
    "                if not comments:\n",
    "                    comments = post.find('li', class_='social-details-social-counts__item--comments')\n",
    "                timestamp = post.find('time', class_='main-feed-activity-card__timestamp')\n",
    "                \n",
    "                comment_count = comment_counts_dict.get(i + 1, 0)\n",
    "                \n",
    "                post_data = {\n",
    "                    'commentary': commentary.text.strip() if commentary else '',\n",
    "                    'reactions': reactions.text.strip() if reactions else '0',\n",
    "                    'comments': str(comment_count) + (' Comments' if comment_count != 1 else ' Comment'),\n",
    "                    'timestamp': timestamp.text.strip() if timestamp else ''\n",
    "                }\n",
    "                company_data['posts'].append(post_data)\n",
    "                logging.info(f\"Extracted post {i+1}: {post_data['commentary'][:50]}... (Timestamp: {post_data['timestamp']}, Comments: {post_data['comments']})\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting posts: {e}\")\n",
    "        \n",
    "        jobs_data = scrape_linkedin_company_jobs(company_name)\n",
    "        company_data['jobs'] = jobs_data\n",
    "                        \n",
    "        website = company_data['overview']['website']\n",
    "        if website:\n",
    "            try:\n",
    "                parsed_url = urlparse(website)\n",
    "                domain = parsed_url.netloc or parsed_url.path.split('/')[0]\n",
    "                if domain.startswith('www.'):\n",
    "                    domain = domain[4:]\n",
    "                \n",
    "                company_data['domain_info']['whois'] = check_whois(domain)\n",
    "                company_data['domain_info']['ssl'] = check_ssl(domain)\n",
    "                company_data['domain_info']['https'] = check_https(website)\n",
    "                company_data['domain_info']['domain_age'] = calculate_domain_age(company_data['domain_info']['whois'])\n",
    "                company_data['domain_info']['website_authenticity'] = check_website_authenticity(website, company_data['overview']['name'])\n",
    "                company_data['domain_info']['engagement'] = calculate_engagement_rate(\n",
    "                    company_data['posts'],\n",
    "                    company_data['overview']['followers']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Domain info processing failed for {website}: {e}\")\n",
    "                company_data['domain_info'] = {\n",
    "                    'whois': {'error': str(e)},\n",
    "                    'ssl': {'error': str(e)},\n",
    "                    'https': {'error': str(e)},\n",
    "                    'domain_age': {'error': str(e)},\n",
    "                    'website_authenticity': {'error': str(e)},\n",
    "                    'engagement': {\n",
    "                        'average_engagement_rate': 0.0,\n",
    "                        'average_reactions': 0.0,\n",
    "                        'average_comments': 0.0,\n",
    "                        'per_post_engagement': []\n",
    "                    }\n",
    "                }\n",
    "        else:\n",
    "            logging.warning(\"No website URL found for domain checks\")\n",
    "            company_data['domain_info']['website_authenticity'] = {'error': 'No website URL provided'}\n",
    "        \n",
    "        return company_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during scraping: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Function to classify company type\n",
    "def classify_company_type(structured_data):\n",
    "    indicators = structured_data['authenticity_indicators']\n",
    "    size = indicators['overview']['company_size']\n",
    "    followers = indicators['overview']['followers']\n",
    "    founded = indicators['overview']['founded']\n",
    "    \n",
    "    try:\n",
    "        size_num = int(re.search(r'\\d+', size.replace(',', '')).group())\n",
    "    except:\n",
    "        size_num = 0\n",
    "    \n",
    "    try:\n",
    "        followers_num = parse_followers_count(followers)\n",
    "    except:\n",
    "        followers_num = 0\n",
    "    \n",
    "    try:\n",
    "        founded_year = int(founded)\n",
    "        current_year = datetime.datetime.now().year\n",
    "        years_since_founded = current_year - founded_year\n",
    "    except:\n",
    "        years_since_founded = float('inf')\n",
    "    \n",
    "    is_small = size_num < 500 or any(s in size.lower() for s in ['1-10', '11-50', '51-200', '201-500'])\n",
    "    is_new = years_since_founded < 5\n",
    "    has_few_followers = followers_num < 10000\n",
    "    \n",
    "    score = 0\n",
    "    if is_new:\n",
    "        score += 2\n",
    "    if has_few_followers:\n",
    "            score += 1\n",
    "    if is_small:\n",
    "        score += 1\n",
    "    \n",
    "    if score >= 3:\n",
    "        return 'startup'\n",
    "    return 'established'\n",
    "\n",
    "# Function to structure authenticity data\n",
    "def structure_authenticity_data(company_data):\n",
    "    structured_data = {\n",
    "        \"company_name\": company_data['overview']['name'],\n",
    "        \"linkedin_url\": \"\",\n",
    "        \"company_type\": \"\",\n",
    "        \"authenticity_indicators\": {\n",
    "            \"domain_info\": {\n",
    "                \"whois\": company_data['domain_info']['whois'],\n",
    "                \"ssl\": company_data['domain_info']['ssl'],\n",
    "                \"https\": company_data['domain_info']['https'],\n",
    "                \"domain_age\": company_data['domain_info']['domain_age'],\n",
    "                \"website_authenticity\": company_data['domain_info']['website_authenticity'],\n",
    "            },\n",
    "            \"overview\": {\n",
    "                \"website\": company_data['overview']['website'],\n",
    "                \"industry\": company_data['overview']['industry'],\n",
    "                \"company_size\": company_data['overview']['company_size'],\n",
    "                \"headquarters\": company_data['overview']['headquarters'],\n",
    "                \"founded\": company_data['overview']['founded'],\n",
    "                \"locations\": company_data['overview']['locations'],\n",
    "                \"followers\": company_data['overview']['followers'],\n",
    "               \n",
    "            },\n",
    "            \"jobs\": {\n",
    "                \"job_count\": company_data['jobs']['job_count'],\n",
    "                \"job_listings\": company_data['jobs']['job_listings'],\n",
    "                \n",
    "            },\n",
    "            \"posts\": {\n",
    "                \"engagement\": company_data['domain_info']['engagement'],\n",
    "                \"post_count\": len(company_data['posts']),\n",
    "                \"post_analysis\": analyze_posts(company_data['posts']),\n",
    "                \n",
    "            }\n",
    "        },\n",
    "        \"authenticity_score\": 0.0,\n",
    "        \"is_likely_authentic\": False\n",
    "    }\n",
    "    return structured_data\n",
    "\n",
    "# Function to validate authenticity\n",
    "def validate_authenticity(structured_data, linkedin_url):\n",
    "    structured_data['linkedin_url'] = linkedin_url\n",
    "    company_type = classify_company_type(structured_data)\n",
    "    structured_data['company_type'] = company_type\n",
    "    indicators = structured_data['authenticity_indicators']\n",
    "    score = 0\n",
    "    validations = {}\n",
    "\n",
    "    whois = indicators['domain_info']['whois']\n",
    "    validations['domain_info'] = {}\n",
    "    if whois.get('domain_name') and isinstance(whois.get('domain_name'), str) and whois['domain_name'].lower() in indicators['overview']['website'].lower():\n",
    "        validations['domain_info']['is_domain_match'] = True\n",
    "        score += 15\n",
    "    reputable_registrars = ['MarkMonitor, Inc.', 'GoDaddy.com, LLC', 'Namecheap, Inc.', 'Cloudflare, Inc.']\n",
    "    if whois.get('registrar') in reputable_registrars:\n",
    "        validations['domain_info']['is_reputable_registrar'] = True\n",
    "        score += 10\n",
    "    try:\n",
    "        creation_date = datetime.datetime.fromisoformat(whois['creation_date'].replace('Z', '')) if whois.get('creation_date') else None\n",
    "        min_age = 0.25 if company_type == 'startup' else 0.5\n",
    "        if creation_date and (datetime.datetime.now() - creation_date).days / 365.25 > min_age:\n",
    "            validations['domain_info']['is_domain_old'] = True\n",
    "            score += 10\n",
    "    except:\n",
    "        validations['domain_info']['is_domain_old'] = False\n",
    "\n",
    "    ssl = indicators['domain_info']['ssl']\n",
    "    trusted_issuers = ['DigiCert', 'Let\\'s Encrypt', 'Sectigo', 'GlobalSign', 'Cloudflare']\n",
    "    if any(issuer in ssl.get('issuer', {}).get('commonName', '') for issuer in trusted_issuers):\n",
    "        validations['domain_info']['is_trusted_issuer'] = True\n",
    "        score += 5\n",
    "    try:\n",
    "        not_after = datetime.datetime.strptime(ssl['not_after'], '%b %d %H:%M:%S %Y GMT')\n",
    "        if not_after > datetime.datetime.now():\n",
    "            validations['domain_info']['is_valid_ssl'] = not_after > datetime.datetime.now()\n",
    "            if not_after:\n",
    "                validations['domain_info']['is_valid_ssl'] = True\n",
    "                score += 5\n",
    "    except:\n",
    "        validations['domain_info']['is_valid_ssl'] = False\n",
    "\n",
    "    website_auth = indicators['domain_info']['website_authenticity']\n",
    "    validations['domain_info']['website_validation'] = {}\n",
    "    if website_auth.get('is_professional'):\n",
    "        validations['domain_info']['website_validation']['is_professional'] = True\n",
    "        score += 5\n",
    "    if website_auth.get('has_contact_info'):\n",
    "        validations['domain_info']['website_validation']['has_contact_info'] = True\n",
    "        score += 5\n",
    "    if not website_auth.get('has_red_flags'):\n",
    "        validations['domain_info']['website_validation']['no_red_flags'] = True\n",
    "        score += 5\n",
    "    if website_auth.get('broken_links_count', 0) == 0:\n",
    "        validations['domain_info']['website_validation']['no_broken_links'] = True\n",
    "        score += 5\n",
    "\n",
    "    validations['overview'] = {}\n",
    "    if indicators['overview']['website'].startswith('http'):\n",
    "        validations['overview']['has_valid_website'] = True\n",
    "        score += 10\n",
    "    if company_type == 'established' and (indicators['overview']['headquarters'] or indicators['overview']['locations']):\n",
    "        validations['overview']['has_physical_presence'] = True\n",
    "        score += 10\n",
    "    elif company_type == 'startup' and (indicators['overview']['headquarters'] or indicators['overview']['locations'] or indicators['overview']['website']):\n",
    "        validations['overview']['has_physical_presence'] = True\n",
    "        score += 10\n",
    "    try:\n",
    "        followers = parse_followers_count(indicators['overview']['followers'])\n",
    "        follower_score = min(20, int(10 * (followers ** 0.5) / 1000))\n",
    "        min_followers = 500 if company_type == 'startup' else 5000\n",
    "        if followers > min_followers:\n",
    "            validations['overview']['has_significant_followers'] = True\n",
    "            score += follower_score\n",
    "    except:\n",
    "        validations['overview']['has_significant_followers'] = False\n",
    "\n",
    "    validations['jobs'] = {}\n",
    "    try:\n",
    "        job_count = int(re.sub(r'[^\\d]', '', indicators['jobs']['job_count'])) if indicators['jobs']['job_count'] else 0\n",
    "        min_jobs = 1 if company_type == 'startup' else 3\n",
    "        if job_count >= min_jobs:\n",
    "            validations['jobs']['has_multiple_jobs'] = True\n",
    "            score += 10 + min(10, job_count)\n",
    "    except:\n",
    "        validations['jobs']['has_multiple_jobs'] = False\n",
    "    recent_jobs = any('ago' in job['posted'] or 'day' in job['posted'] or 'week' in job['posted'] for job in indicators['jobs']['job_listings'])\n",
    "    if recent_jobs:\n",
    "        validations['jobs']['has_active_jobs'] = True\n",
    "        score += 10\n",
    "\n",
    "    validations['posts'] = {}\n",
    "    min_posts = 3 if company_type == 'startup' else 5\n",
    "    if indicators['posts']['post_count'] >= min_posts:\n",
    "        validations['posts']['has_recent_posts'] = True\n",
    "        score += 5\n",
    "    professional_threshold = 0.4 if company_type == 'startup' else 0.6\n",
    "    professional_posts = sum(1 for post in indicators['posts']['post_analysis'] if post['is_professional'] and post['sentiment'] == 'positive')\n",
    "    if professional_posts / max(1, len(indicators['posts']['post_analysis'])) > professional_threshold:\n",
    "        validations['posts']['has_professional_content'] = True\n",
    "        score += 5\n",
    " \n",
    "    structured_data['authenticity_score'] = score\n",
    "    authenticity_threshold = 50 if company_type == 'startup' else 60\n",
    "    structured_data['is_likely_authentic'] = score >= authenticity_threshold\n",
    "    structured_data['authenticity_indicators']['validation'] = validations\n",
    "    return structured_data\n",
    "\n",
    "# Function to scrape  leaders, and affiliated pages\n",
    "def scrape_linkedin_sections(linkedin_url):\n",
    "    if not validate_linkedin_url(linkedin_url):\n",
    "        logging.error(\"Invalid LinkedIn company URL\")\n",
    "        return None\n",
    "    \n",
    "    company_name = linkedin_url.split('/company/')[-1].split('/')[0].replace('-', ' ').title()\n",
    "    \n",
    "    scraped_data = {\n",
    "        'company_name': company_name,\n",
    "        'linkedin_url': linkedin_url,\n",
    "        'leaders': [],\n",
    "        'affiliated_pages': []\n",
    "    }\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        life_url = f\"{linkedin_url.rstrip('/')}/life\"\n",
    "        logging.info(f\"Navigating to {life_url}\")\n",
    "        driver.get(life_url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                \n",
    "        leaders_section = soup.find('section', attrs={'data-test-id': 'leaders-at'})\n",
    "        if leaders_section:\n",
    "            leaders_list = leaders_section.find_all('li')\n",
    "            for leader in leaders_list:\n",
    "                name_elem = leader.find('h3', class_='base-main-card__title')\n",
    "                title_elem = leader.find('h4', class_='base-main-card__subtitle')\n",
    "                profile_link = leader.find('a', class_='base-card--link')\n",
    "                \n",
    "                leader_data = {\n",
    "                    'name': name_elem.text.strip() if name_elem else '',\n",
    "                    'title': title_elem.text.strip() if title_elem else '',\n",
    "                    'profile_url': profile_link['href'] if profile_link else ''\n",
    "                }\n",
    "                if leader_data['name'] and leader_data['profile_url']:\n",
    "                    scraped_data['leaders'].append(leader_data)\n",
    "                    logging.info(f\"Extracted leader: {leader_data['name']}, {leader_data['title']}\")\n",
    "        \n",
    "        affiliated_section = soup.find('section', attrs={'data-test-id': 'affiliated-pages'})\n",
    "        if affiliated_section:\n",
    "            affiliated_list = affiliated_section.find_all('li')\n",
    "            for affiliate in affiliated_list:\n",
    "                name_elem = affiliate.find('h3', class_='base-aside-card__title')\n",
    "                subtitle_elem = affiliate.find('p', class_='base-aside-card__subtitle')\n",
    "                location_elem = affiliate.find('p', class_='base-aside-card__second-subtitle')\n",
    "                link_elem = affiliate.find('a', class_='base-aside-card--link')\n",
    "                \n",
    "                affiliate_data = {\n",
    "                    'name': name_elem.text.strip() if name_elem else '',\n",
    "                    'subtitle': subtitle_elem.text.strip() if subtitle_elem else '',\n",
    "                    'location': location_elem.text.strip() if location_elem else '',\n",
    "                    'url': link_elem['href'] if link_elem else ''\n",
    "                }\n",
    "                if affiliate_data['name'] and affiliate_data['url']:\n",
    "                    scraped_data['affiliated_pages'].append(affiliate_data)\n",
    "                    logging.info(f\"Extracted affiliated page: {affiliate_data['name']}\")\n",
    "        \n",
    "        return scraped_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during scraping: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Function to process user-provided LinkedIn URL for both authenticity check and sections scraping\n",
    "def check_and_scrape_linkedin_company():\n",
    "    try:\n",
    "        linkedin_url = input(\"Enter the LinkedIn company URL (e.g., https://www.linkedin.com/company/yahoo/): \").strip()\n",
    "        \n",
    "        if not validate_linkedin_url(linkedin_url):\n",
    "            print(\"Invalid LinkedIn company URL. Please provide a valid URL like https://www.linkedin.com/company/company-name/\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Scraping company data for authenticity check...\")\n",
    "        company_data = scrape_linkedin_company_from_url(linkedin_url)\n",
    "        if not company_data:\n",
    "            logging.error(\"Failed to scrape company data for authenticity\")\n",
    "            print(\"Failed to scrape company data for authenticity. Check logs for details.\")\n",
    "            return None\n",
    "        \n",
    "        structured_data = structure_authenticity_data(company_data)\n",
    "        \n",
    "        print(\"Validating authenticity...\")\n",
    "        validated_data = validate_authenticity(structured_data, linkedin_url)\n",
    "        \n",
    "        print(\"Scraping leaders, and affiliated pages...\")\n",
    "        sections_data = scrape_linkedin_sections(linkedin_url)\n",
    "        if not sections_data:\n",
    "            logging.error(\"Failed to scrape leaders, and affiliated pages\")\n",
    "            print(\"Failed to scrape leaders, and affiliated pages. Check logs for details.\")\n",
    "            sections_data = {\n",
    "                'company_name': validated_data['company_name'],\n",
    "                'linkedin_url': linkedin_url,\n",
    "                'leaders': [],\n",
    "                'affiliated_pages': []\n",
    "            }\n",
    "        \n",
    "        # Combine both data sets into a single dictionary\n",
    "        combined_data = {\n",
    "            'authenticity': validated_data,\n",
    "            'sections': sections_data\n",
    "        }\n",
    "        \n",
    "        # Save combined data to a single JSON file\n",
    "        company_name = validated_data['company_name'].lower().replace(' ', '_')\n",
    "        output_file = f\"{company_name}_company_data.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n=== Analysis for {validated_data['company_name']} ===\")\n",
    "        print(f\"LinkedIn URL: {validated_data['linkedin_url']}\")\n",
    "        print(\"\\n--- Authenticity Check Results ---\")\n",
    "        print(f\"Company Type: {validated_data['company_type'].capitalize()}\")\n",
    "        print(f\"Founded: {validated_data['authenticity_indicators']['overview']['founded']}\")\n",
    "        print(f\"Followers: {validated_data['authenticity_indicators']['overview']['followers']}\")\n",
    "        print(f\"Employee Count: {validated_data['authenticity_indicators']['overview']['company_size']}\")\n",
    "        print(f\"Job Count: {validated_data['authenticity_indicators']['jobs']['job_count']}\")\n",
    "        print(f\"Post Count: {validated_data['authenticity_indicators']['posts']['post_count']}\")\n",
    "        print(f\"Average Engagement Rate: {validated_data['authenticity_indicators']['posts']['engagement']['average_engagement_rate']}%\")\n",
    "        print(f\"Average Reactions per Post: {validated_data['authenticity_indicators']['posts']['engagement']['average_reactions']}\")\n",
    "        print(f\"Average Comments per Post: {validated_data['authenticity_indicators']['posts']['engagement']['average_comments']}\")\n",
    "        print(f\"Website Authenticity Score: {validated_data['authenticity_indicators']['domain_info']['website_authenticity']['authenticity_score']}/20\")\n",
    "        print(f\"Website Professional: {validated_data['authenticity_indicators']['domain_info']['website_authenticity']['is_professional']}\")\n",
    "        print(f\"Website Has Contact Info: {validated_data['authenticity_indicators']['domain_info']['website_authenticity']['has_contact_info']}\")\n",
    "        print(f\"Website Has Red Flags: {validated_data['authenticity_indicators']['domain_info']['website_authenticity']['has_red_flags']}\")\n",
    "        print(f\"Website Broken Links: {validated_data['authenticity_indicators']['domain_info']['website_authenticity']['broken_links_count']}\")\n",
    "        print(f\"Authenticity Score: {validated_data['authenticity_score']}/150\")\n",
    "        print(f\"Is Likely Authentic: {validated_data['is_likely_authentic']}\")\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "       \n",
    "        return combined_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Analysis failed: {e}\")\n",
    "        print(f\"Error: {str(e)}. Check logs for details.\")\n",
    "        return None\n",
    "\n",
    "# Main entry point\n",
    "if __name__ == \"__main__\":\n",
    "    result = check_and_scrape_linkedin_company()\n",
    "    if result:\n",
    "        print(\"\\nCompany analysis completed successfully.\")\n",
    "    else:\n",
    "        print(\"\\nCompany analysis failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a000165-d885-4540-891e-56cf3fc21a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
